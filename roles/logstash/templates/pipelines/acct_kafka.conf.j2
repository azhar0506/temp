# ====================================
# Main Logstash pipeline that collects WBC & MNA ACCT data from Kafka, then sends it to single Elasticsearch index (one index for both WBC & MNA data)
# There are 3 important enrichments happening:
#   1. We add `type` field, to distinguish between WBC & MNA data in Elasticsearch index
#   2. We add `network_type` field to understand what network customer is using (21C or 20C)
#   3. We extract domain information `radius_username` into `user_realm`. This one helps to map customer to the CP given you have domain to CP map available 
# ====================================


input {
  {%+ for input in item.kafka_inputs %}
kafka {
    type              => "{{ input.type }}"
    bootstrap_servers => "{{ input.kafka_bootstrap_servers | default("default") }}"
    group_id          => "{{ input.pipeline_group_id       | default("default") }}"
    client_id         => "{{ input.pipeline_client_id      | default("default") }}"
    topics            => {{  input.kafka_topics            | default('["wbc_default"]') }}

    consumer_threads           => {{ logstash_pipeline_consumer_threads | default(2) }}
    sasl_kerberos_service_name => "kafka"
    security_protocol          => "SASL_PLAINTEXT"
    jaas_path                  => "{{ logstash_jaas_path | default("/usr/share/config/jaas.conf") }}"
    kerberos_config            => "{{ logstash_kerberos_config_path | default("/usr/share/config/default.conf") }}"
    auto_offset_reset          => "latest"

    client_dns_lookup => "use_all_dns_ips"
  }
  {% endfor %}

}

filter {
  if [type] == "wbc" {
    csv {

      columns => ["server_host_name","acct_status_type","session_id","calling_station_id","accounting_event_time","accounting_event_delay_time","radius_username","ip_addr","ipv6_addr","nas_ip_address","cumulative_duration_secs","acct_terminate_cause","acct_interim_reason","nas_port","nas_port_id","input_octets","input_gigawords","input_packets","output_octets","output_gigawords","output_packets","data_rate_upstream","data_rate_downstream","profile_id","openreach_id"]

      separator => ","
      convert => {
        "accounting_event_time" => "date"
      }

      convert => {
        "accounting_event_delay_time" => "integer"
        "cumulative_duration_secs"    => "integer"
        "input_octets"                => "integer"
        "input_gigawords"             => "integer"
        "input_packets"               => "integer"
        "output_octets"               => "integer"
        "output_gigawords"            => "integer"
        "output_packets"              => "integer"
        "data_rate_upstream"          => "integer"
        "data_rate_downstream"        => "integer"
      }
    }

    date {
      match => [ "accounting_event_time", "yyyy/MM/dd HH:mm:ss" ]
    }
  }

  if [type] == "mna" {
    csv {

      columns => ["server_host_name","acct_status_type","session_id","calling_station_id","accounting_event_time","accounting_event_delay_time","radius_username","acct_authentic","ip_addr","ip_addr_netmask","ip_framed_route","framed_pool","framed_protocol","class","ipv6_addr","nas_ip_address","cumulative_duration_secs","acct_terminate_cause","acct_interim_reason","nas_port","nas_port_id","nas_port_type","input_octets","input_gigawords","input_packets","output_octets","output_gigawords","output_packets","openreach_id","tunnel_client_endpoint","tunnel_server_endpoint","filter_id"]
      
      separator => ","
      convert => {
        "accounting_event_time" => "date"
      }

      convert => {
        "accounting_event_delay_time" => "integer"
        "cumulative_duration_secs"    => "integer"
        "input_octets"                => "integer"
        "input_gigawords"             => "integer"
        "input_packets"               => "integer"
        "output_octets"               => "integer"
        "output_gigawords"            => "integer"
        "output_packets"              => "integer"
      }
    }

    date {
      match => [ "accounting_event_time", "yyyy/MM/dd HH:mm:ss" ]
    }

    # Raw tunnel attributes look ugly, so we need to parse them to make them searchable in Elasticsearch
    ruby {
      path => "/usr/share/logstash/pipeline/parse_tunnel_attr.rb"
    }
  }

  # Sometimes Logstash reads in old messages from Kafka topics and that breaks indexing into Elasticsearch
  # because of ILM policy that makes indices of 2 days old read-only
  ruby {
    init => "require 'time'"
    code => "
            if ( Time.now.to_i - Time.parse(event.get('accounting_event_time')).to_i ) > 172800
              event.cancel
            end
          "
  }

  # Ensure that calling_station_id is always uppercase as there are moments when the value can contain lowercase characters
  mutate {
    uppercase => [ "calling_station_id" ]
  }
  # Enreach record with network_type - useful when we need to see how many customers there are on 20C or 21C network
  if [calling_station_id] =~ /BBIP.*|FTIP.*/ {
    mutate { add_field => { "network_type" => "20C" } }
  } else if [calling_station_id] =~ /BBEU.*/ {
    mutate { add_field => { "network_type" => "21C" } }
  } else if [calling_station_id] =~ /SELENIUM.*/ {
    mutate { add_field => { "network_type" => "heartbeat" } }
  } else {
    mutate { add_field => { "network_type" => "unknown" } }
  }

  # Extract domain (aka realm) from radius_username
  ruby {
    code => "realm = event.get('radius_username').strip.reverse.split('@'); realm.length <= 1 ? event.set('user_realm', 'unknown') : event.set('user_realm', realm[0].reverse)"
  }

  # We need to do some Data Quality checks before inserting data into Elasticsearch
  # This way it would be much easier to keep track of it and build visualisation/alerts
  # to ensure any Data Quality issue is proactively detected and tackled
  ruby {
    path => "/usr/share/logstash/pipeline/add_dq_meta.rb"
  }


  mutate {
    remove_field => [ "message" ]
  }
}

output {
  elasticsearch {
    hosts    => {{ logstash_output_elasticsearch_hosts | default("") }}
    index    => "{{ logstash_acct_index_name_w_date | default("acct-%{+YYYY.MM.dd}") }}"
    user     => "{{ logstash_output_elasticsearch_username | default("elastic") }}"
    password => "{{ logstash_output_elasticsearch_password | default("no_password") }}"

    ssl_enabled                 => true
    ssl_certificate_authorities => "{{ logstash_output_elasticsearch_ssl_certificate_authority | default("/usr/share/logstash/ca.crt") }}"
  }
}
