# ====================================
# Logstash pipeline that collects only WBC ACCT data from Kafka, then adds geolocation based on customer's IP address (`ip_addr` or `ipv6_addr`) and finally sends data to Elasticsearch index
# If IP address is not available, then the record is dropped
#
# On top of that, there are also 2 data enrichments happening:
#   1. We add `network_type` field to understand what network customer is using (21C or 20C)
#   2. We extract domain information `radius_username` into `user_realm`. This one helps to map customer to the CP given you have domain to CP map available 
#
# And the last thing to note - this pipeline updates(overwrites) old records in the Elasticsearch index, thus we only have ~10mln records in the index with most up-to-date geolocation for the customer
# ====================================


input {
  kafka {
    type              => "geoip-data"
    bootstrap_servers => "{{ item.kafka_bootstrap_servers | default("default") }}"
    group_id          => "{{ item.pipeline_group_id | default("default") }}"
    client_id         => "{{ item.pipeline_client_id | default("default") }}"
    topics            => {{  item.kafka_topics | default('["wbc_default"]') }}

    consumer_threads           => {{ logstash_pipeline_consumer_threads | default(2) }}
    sasl_kerberos_service_name => "kafka"
    security_protocol          => "SASL_PLAINTEXT"
    jaas_path                  => "{{ logstash_jaas_path | default("/usr/share/config/jaas.conf") }}"
    kerberos_config            => "{{ logstash_kerberos_config_path | default("/usr/share/config/default.conf") }}"
    auto_offset_reset          => "latest"

    client_dns_lookup => "use_all_dns_ips"
  }
}

filter {
  csv {
    columns => ["server_host_name","acct_status_type","session_id","calling_station_id","accounting_event_time","accounting_event_delay_time","radius_username","ip_addr","ipv6_addr","nas_ip_address","cumulative_duration_secs","acct_terminate_cause","acct_interim_reason","nas_port","nas_port_id","input_octets","input_gigawords","input_packets","output_octets","output_gigawords","output_packets","data_rate_upstream","data_rate_downstream","profile_id","openreach_id"]
    separator => ","
    convert => { "accounting_event_time" => "date" }
  }

  mutate {
    lowercase => [ "ip_addr", "ipv6_addr" ]
  }
  if [ip_addr] == "none" and [ipv6_addr] == "none" {
    drop {}
  } else {
    if [ip_addr] != "none" {
      geoip {
        source     => "ip_addr"
        cache_size => 6000
      }
    } else if [ipv6_addr] != "none" and [ipv6_addr] != "0000:0000:0000:0000:0000:0000:0000:0000" {
      dissect {
        mapping => {
          "ipv6_addr" => "%{cleaned_ipv6_addr}/%{}"
        }
      }
      geoip {
        source     => "cleaned_ipv6_addr"
        cache_size => 6000
      }
      mutate {
        remove_field => ["cleaned_ipv6_addr"]
      }
    }

    mutate {
      remove_field => ["message","accounting_event_delay_time","cumulative_duration_secs","acct_terminate_cause","acct_interim_reason","nas_port","input_octets","input_gigawords","input_packets","output_octets","output_gigawords","output_packets","data_rate_upstream","data_rate_downstream","profile_id","openreach_id"]
    }

    mutate {
      uppercase => [ "calling_station_id" ]
    }
    if [calling_station_id] =~ /BBIP.*|FTIP.*/ {
      mutate { add_field => { "network_type" => "20C" } }
    } else if [calling_station_id] =~ /BBEU.*/ {
      mutate { add_field => { "network_type" => "21C" } }
    } else if [calling_station_id] =~ /SELENIUM.*/ {
      mutate { add_field => { "network_type" => "heartbeat" } }
    } else {
      mutate { add_field => { "network_type" => "unknown" } }
    }

    ruby {
      code => "realm = event.get('radius_username').strip.reverse.split('@'); realm.length <= 1 ? event.set('user_realm', 'unknown') : event.set('user_realm', realm[0].reverse)"
    }
    mutate {
      remove_field => ["radius_username"]
    }

    mutate {
      copy => { "accounting_event_time" => "version_id"  }
    }
    ruby {
      init => "require 'date'"
      code => "event.set('version_id', DateTime.parse(event.get('version_id')).to_time.to_i)"
    }
  }
}

output {
  elasticsearch {
    hosts             => {{ logstash_output_elasticsearch_hosts | default("") }}
    action            => "index"
    index             => "{{ logstash_geoip_data_index_name | default("geoip_data") }}"
    document_id       => "%{calling_station_id}"
    version           => "%{version_id}"
    version_type      => "external_gt"
    retry_on_conflict => 0

    user     => "{{ logstash_output_elasticsearch_username | default("elastic") }}"
    password => "{{ logstash_output_elasticsearch_password | default("no_password") }}"

    ssl_enabled                 => true
    ssl_certificate_authorities => "{{ logstash_output_elasticsearch_ssl_certificate_authority | default("/usr/share/logstash/ca.crt") }}"
  }
}
