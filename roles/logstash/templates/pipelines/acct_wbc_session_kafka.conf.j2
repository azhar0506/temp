# ====================================
# Logstash pipeline that collects only WBC ACCT data from Kafka, then sends data to Elasticsearch index
# There are also 3 data modifications happening:
#   1. We add `network_type` field to understand what network customer is using (21C or 20C)
#   2. We extract domain information `radius_username` into `user_realm`. This one helps to map customer to the CP given you have domain to CP map available 
#   3. We remove `radius_username` as this one is not needed in this index
#
# And the last thing to note - this pipeline updates(overwrites) old records in the Elasticsearch index, thus we only have ~10mln records in the index with most up-to-date information on the customer's session (is it active or not)
# ====================================


input {
  kafka {
    type              => "session-data"
    bootstrap_servers => "{{ item.kafka_bootstrap_servers | default("default") }}"
    group_id          => "{{ item.pipeline_group_id | default("default") }}"
    client_id         => "{{ item.pipeline_client_id | default("default") }}"
    topics            => {{  item.kafka_topics | default('["wbc_default"]') }}

    consumer_threads           => {{ logstash_pipeline_consumer_threads | default(2) }}
    sasl_kerberos_service_name => "kafka"
    security_protocol          => "SASL_PLAINTEXT"
    jaas_path                  => "{{ logstash_jaas_path | default("/usr/share/config/jaas.conf") }}"
    kerberos_config            => "{{ logstash_kerberos_config_path | default("/usr/share/config/default.conf") }}"
    auto_offset_reset          => "latest"

    client_dns_lookup => "use_all_dns_ips"
  }
}

filter {
  csv {
    columns => ["server_host_name","acct_status_type","session_id","calling_station_id","accounting_event_time","accounting_event_delay_time","radius_username","ip_addr","ipv6_addr","nas_ip_address","cumulative_duration_secs","acct_terminate_cause","acct_interim_reason","nas_port","nas_port_id","input_octets","input_gigawords","input_packets","output_octets","output_gigawords","output_packets","data_rate_upstream","data_rate_downstream","profile_id","openreach_id"]
    separator => ","
    convert => {
      "accounting_event_time"    => "date"
      "cumulative_duration_secs" => "integer"
    }
  }

  mutate {
    remove_field => ["message","accounting_event_delay_time","acct_terminate_cause","acct_interim_reason","input_octets","input_gigawords","input_packets","output_octets","output_gigawords","output_packets","data_rate_upstream","data_rate_downstream","profile_id","openreach_id"]
  }

  mutate {
    uppercase => [ "calling_station_id" ]
  }
  if [calling_station_id] =~ /BBIP.*|FTIP.*/ {
    mutate { add_field => { "network_type" => "20C" } }
  } else if [calling_station_id] =~ /BBEU.*/ {
    mutate { add_field => { "network_type" => "21C" } }
  } else if [calling_station_id] =~ /SELENIUM.*/ {
    mutate { add_field => { "network_type" => "heartbeat" } }
  } else {
    mutate { add_field => { "network_type" => "unknown" } }
  }

  ruby {
    code => "realm = event.get('radius_username').strip.reverse.split('@'); realm.length <= 1 ? event.set('user_realm', 'unknown') : event.set('user_realm', realm[0].reverse)"
  }
  mutate {
    remove_field => ["radius_username"]
  }

  mutate {
    copy => { "accounting_event_time" => "version_id"  }
  }
  ruby {
    init => "require 'date'"
    code => "event.set('version_id', DateTime.parse(event.get('version_id')).to_time.to_i)"
  }

  translate {
    field            => "[nas_ip_address]"
    destination      => "[nas_ip_location]"
    dictionary_path  => "/usr/share/logstash/pipeline/nas_ip_address_2_es_geo_point.json"
    refresh_interval => 3600
  }
}

output {
  elasticsearch {
    hosts             => {{ logstash_output_elasticsearch_hosts | default("") }}
    action            => "index"
    index             => "{{ logstash_session_data_index_name | default("session-data") }}"
    document_id       => "%{calling_station_id}"
    version           => "%{version_id}"
    version_type      => "external_gt"
    retry_on_conflict => 0

    user     => "{{ logstash_output_elasticsearch_username | default("elastic") }}"
    password => "{{ logstash_output_elasticsearch_password | default("no_password") }}"

    ssl_enabled                 => true
    ssl_certificate_authorities => "{{ logstash_output_elasticsearch_ssl_certificate_authority | default("/usr/share/logstash/ca.crt") }}"
  }
}
